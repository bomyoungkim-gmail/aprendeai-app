"""WHAT_IF_SCENARIO"""
from typing import List, Dict, Any
import logging
from games.base import BaseGame

logger = logging.getLogger(__name__)

class WhatIfScenarioGame(BaseGame):
    """What-If Scenario - Predict consequences"""
    
    GAME_ID = "WHAT_IF_SCENARIO"
    GAME_NAME = "CenÃ¡rio E Se?"
    GAME_INTENT = "application"
    REQUIRES_CONTENT = True
    DIFFICULTY_RANGE = (2, 4)
    DURATION_MIN = 5
    
    def create_round(self, state: Dict[str, Any], difficulty: int) -> Dict[str, Any]:
        difficulty = self.validate_difficulty(difficulty)
        concept = state.get('concept', 'gravity')
        scenario = f"E se a {concept} da Terra diminuÃ­sse pela metade?"
        
        prompt = f"""ðŸ”® **CenÃ¡rio E Se?**

{scenario}

Liste pelo menos 3 consequÃªncias cientÃ­ficas deste cenÃ¡rio hipotÃ©tico."""
        
        return {
            'game_mode': self.GAME_ID,
            'prompt': prompt,
            'difficulty': difficulty,
            'data': {
                'scenario': scenario,
                'key_consequences': ['atmosphere', 'orbit', 'weight']
            }
        }

    async def evaluate_answer(self, round_data: Dict[str, Any], answer: str) -> Dict[str, Any]:
        """Evaluate prediction quality with LLM"""
        scenario = round_data['data']['scenario']
        
        try:
            return await self._llm_evaluate(scenario, answer)
        except Exception as e:
            logger.warning(f"LLM failed: {e}")
            return self._heuristic_evaluate(round_data, answer)
    
    async def _llm_evaluate(self, scenario: str, predictions: str) -> Dict[str, Any]:
        """LLM evaluates prediction quality"""
        if not self.llm_service:
            raise ValueError("LLM not available")
        
        prompt = f"""Avalie as previsÃµes para este cenÃ¡rio hipotÃ©tico:

CENÃRIO: {scenario}
PREVISÃ•ES: {predictions}

CritÃ©rios (0-100):
1. Cientificamente plausÃ­veis? (40%)
2. AbrangÃªncia (mÃºl tiplas consequÃªncias)? (30%)
3. Profundidade de anÃ¡lise? (30%)

JSON:
- score (0-100)
- feedback
- plausibility ("baixa", "mÃ©dia", "alta")
- count_consequences (nÃºmero)"""

        result = await self.llm_service.predict_json(
            prompt=prompt,
            schema={
                "type": "object",
                "properties": {
                    "score": {"type": "number", "minimum": 0, "maximum": 100},
                    "feedback": {"type": "string"},
                    "plausibility": {"type": "string", "enum": ["baixa", "mÃ©dia", "alta"]},
                    "count_consequences": {"type": "integer"}
                },
                "required": ["score", "feedback"]
            },
            temperature=0.7
        )
        
        score = result.get("score", 0)
        
        return {
            'score': int(score * 1.2),  # Scale to 120
            'max_score': 120,
            'feedback': f"ðŸ”® {result.get('feedback', 'PrevisÃµes analisadas!')}",
            'correct': score >= 70,
            'breakdown': {
                'llm_score': score,
                'plausibility': result.get('plausibility', 'mÃ©dia'),
                'consequences_found': result.get('count_consequences', 0)
            }
        }
    
    def _heuristic_evaluate(self, round_data: Dict[str, Any], answer: str) -> Dict[str, Any]:
        """Fallback heuristic"""
        consequences = round_data['data']['key_consequences']
        found = [c for c in consequences if any(word in answer.lower() for word in c.split())]
        coverage = len(found) / len(consequences)
        
        score = int(coverage * 120)
        
        return {
            'score': score,
            'max_score': 120,
            'feedback': f"ðŸ”® VocÃª identificou {len(found)}/{len(consequences)} consequÃªncias-chave!",
            'correct': coverage >= 0.6,
            'breakdown': {'coverage': coverage, 'found': found, 'method': 'heuristic'}
        }
